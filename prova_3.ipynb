{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Implementazione di una Pipeline RAG con FAISS e LangChain\n",
    "\n",
    "Questo notebook illustra la costruzione di una pipeline **Retrieval-Augmented Generation (RAG)** utilizzando:\n",
    "\n",
    "- **FAISS** per l'archiviazione e il recupero di embedding vettoriali.\n",
    "- **LangChain** per il caricamento di documenti, il processamento del testo e l'interrogazione di un LLM.\n",
    "- **Ollama** come modello LLaMA 3.2 per la generazione delle risposte.\n",
    "- **Hugging Face** per la generazione degli embedding con `sentence-transformers/all-mpnet-base-v2`.\n",
    "\n",
    "Ogni sezione include il codice e una spiegazione dettagliata.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 1. Importazione delle librerie\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Analisi riga per riga\n",
    "- **`PyPDFLoader`**: Carica e legge documenti PDF.\n",
    "- **`CharacterTextSplitter`**: Suddivide il testo del PDF in segmenti piÃ¹ piccoli (**chunk**).\n",
    "- **`HuggingFaceEmbeddings`**: Genera embedding numerici per i chunk di testo.\n",
    "- **`FAISS`**: Database vettoriale che salva gli embedding e permette la ricerca semantica.\n",
    "- **`OllamaLLM`**: Interfaccia per interrogare il modello **LLaMA 3.2**.\n",
    "- **`RetrievalQA`**: Unisce retrieval e generazione in una pipeline automatizzata.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- **FAISS** archivia gli embedding e supporta la ricerca vettoriale.\n",
    "- **Hugging Face** genera gli embedding per confrontare documenti simili.\n",
    "- **LangChain** gestisce il caricamento dei documenti e la suddivisione in chunk.\n",
    "- **Ollama** elabora le domande dell'utente e genera risposte in linguaggio naturale.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 2. Caricamento del documento\n",
    "\n",
    "```python\n",
    "# Load the document\n",
    "loader = PyPDFLoader(\"Foundations of LLMs.pdf\")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **`PyPDFLoader(\"Foundations of LLMs.pdf\")`** carica il file PDF.\n",
    "- **`documents = loader.load()`** legge il testo e lo trasforma in una lista di oggetti LangChain.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Estrae contenuto da un documento per poterlo indicizzare e successivamente interrogare.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 3. Suddivisione del documento in chunk\n",
    "\n",
    "```python\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\n",
    "\")\n",
    "docs = text_splitter.split_documents(documents=documents)\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **`chunk_size=1000`**: Divide il testo in blocchi da **1000 caratteri**.\n",
    "- **`chunk_overlap=30`**: Sovrappone i chunk di **30 caratteri** per mantenere il contesto.\n",
    "- **`separator=\"\n",
    "\"`**: Divide il testo basandosi sui ritorni a capo (`\n",
    "`).\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Suddividere il documento Ã¨ essenziale per la ricerca semantica: aiuta FAISS a gestire e recuperare informazioni rilevanti.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 4. Creazione degli embedding con Hugging Face\n",
    "\n",
    "```python\n",
    "# âœ… Impostiamo il modello per funzionare sulla CPU\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}  # ðŸ”´ Cambia \"cuda\" in \"cpu\"\n",
    "\n",
    "# âœ… Caricamento del modello\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **`sentence-transformers/all-mpnet-base-v2`** Ã¨ un modello ottimizzato per il **retrieval semantico**.\n",
    "- **`device: \"cpu\"`** imposta il modello per funzionare sulla CPU.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Gli embedding permettono di **trasformare il testo in vettori numerici**, essenziali per la ricerca nei database vettoriali.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 5. Creazione dello store vettoriale FAISS\n",
    "\n",
    "```python\n",
    "# âœ… Creazione dello store vettoriale FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **`FAISS.from_documents(docs, embeddings)`**:\n",
    "  - Converte ogni chunk in un embedding.\n",
    "  - Li salva in una struttura ottimizzata per la ricerca vettoriale.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- FAISS permette di **ricercare il documento piÃ¹ simile a una query** basandosi sulla distanza vettoriale.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 6. Salvataggio e caricamento dello store FAISS\n",
    "\n",
    "```python\n",
    "# âœ… Salvataggio e caricamento dello store vettoriale\n",
    "vectorstore.save_local(\"faiss_index_\")\n",
    "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **`save_local(\"faiss_index_\")`** salva FAISS su disco per un uso futuro.\n",
    "- **`load_local(\"faiss_index_\")`** ricarica FAISS in memoria.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Permette di **evitare il ricalcolo degli embedding**, ottimizzando le prestazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 7. Creazione del retriever\n",
    "\n",
    "```python\n",
    "retriever = persisted_vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- Converte FAISS in un **retriever**, permettendo di recuperare documenti in base a una query.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Gestisce lâ€™interrogazione del database vettoriale per trovare i documenti piÃ¹ pertinenti.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 8. Inizializzazione del modello LLaMA\n",
    "\n",
    "```python\n",
    "# Initialize the LLaMA model\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- Carica il modello **LLaMA 3.2** tramite Ollama.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- **Genera le risposte basandosi sulle informazioni recuperate dal retriever**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 9. Creazione della pipeline RAG con `RetrievalQA`\n",
    "\n",
    "```python\n",
    "# âœ… Crea il modello di QA con il retriever\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **Unisce retrieval e generazione** in una pipeline automatizzata.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Recupera documenti rilevanti **e costruisce il prompt** per il modello LLaMA automaticamente.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 10. Loop interattivo per interrogare il sistema\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    query = input(\"Type your query (or type 'Exit' to quit): \n",
    "\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    # ðŸ”´ Sostituire .run() con .invoke()\n",
    "    result = qa.invoke(query)  # âœ… Metodo corretto\n",
    "    print(result)\n",
    "```\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- **Chiede all'utente una domanda**.\n",
    "- **Recupera i documenti pertinenti**.\n",
    "- **Passa il prompt a LLaMA** e **genera una risposta**.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Permette di **interrogare il modello** con domande in linguaggio naturale.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸš€ **Ora la pipeline Ã¨ pronta per essere testata!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load the document\n",
    "loader = PyPDFLoader(\"Foundations of LLMs.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# âœ… Impostiamo il modello per funzionare sulla CPU\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}  # ðŸ”´ Cambia \"cuda\" in \"cpu\"\n",
    "\n",
    "# âœ… Caricamento del modello\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# âœ… Creazione dello store vettoriale FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# âœ… Salvataggio e caricamento dello store vettoriale\n",
    "vectorstore.save_local(\"faiss_index_\")\n",
    "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# âœ… Creazione del retriever\n",
    "retriever = persisted_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the LLaMA model\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# Test with a sample prompt\n",
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is long sequence modeling?', 'result': 'Long sequence modeling refers to the process of analyzing or generating sequences that are significantly longer than what can be accommodated by traditional models. In the context of language models, this often involves dealing with very large input sequences, such as text documents or conversations, where each token represents a word or character in the sequence.'}\n",
      "{'query': 'what is model ensembling?', 'result': 'According to the context provided, Model Ensembling (option a) refers to a method where multiple LLMs varying in architectures or parameters are used. Each LLM receives the same prompt and produces a prediction, which are then combined to generate the final prediction.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# âœ… Crea il modello di QA con il retriever\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# âœ… Loop interattivo per interrogare il sistema\n",
    "while True:\n",
    "    query = input(\"Type your query (or type 'Exit' to quit): \\n\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    # ðŸ”´ Sostituire .run() con .invoke()\n",
    "    result = qa.invoke(query)  # âœ… Metodo corretto\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_prove)",
   "language": "python",
   "name": "rag_prove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
