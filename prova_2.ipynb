{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Implementazione di una Pipeline RAG con ChromaDB e Ollama\n",
    "\n",
    "Questo notebook illustra passo dopo passo la costruzione di una pipeline **Retrieval-Augmented Generation (RAG)** usando **ChromaDB** per la ricerca semantica e **Ollama** per la generazione del testo.\n",
    "\n",
    "Ogni sezione include codice Python e una spiegazione dettagliata.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importazione delle librerie\n",
    "\n",
    "### ðŸ“Œ Analisi riga per riga\n",
    "- `OllamaEmbeddings`: Crea embedding numerici per i documenti e le query.\n",
    "- `OllamaLLM`: Interfaccia per interrogare il modello LLM di Ollama.\n",
    "- `chromadb`: Libreria per il database vettoriale ChromaDB.\n",
    "- `os`: Permette di gestire i percorsi dei file e la persistenza del database.\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- **LangChain-Ollama** genera embedding e risposte testuali.\n",
    "- **ChromaDB** archivia gli embedding e supporta la ricerca vettoriale.\n",
    "- **OS** Ã¨ necessario per salvare il database in locale.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Definizione del modello LLM\n",
    "\n",
    "### ðŸ“Œ Ruolo nel sistema RAG\n",
    "- Indica il modello LLM che verrÃ  usato **sia per il calcolo degli embedding che per la generazione delle risposte**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Configurazione del database ChromaDB\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- Inizializza **ChromaDB con storage persistente** in `chroma_db/`.\n",
    "- Se `chroma_db/` non esiste, lo crea.\n",
    "- I dati verranno memorizzati in SQLite (`chroma.sqlite3`) e in **file binari separati per gli embedding**.\n",
    "\n",
    "### ðŸ“Œ Struttura fisica generata\n",
    "| **File/Cartella** | **Descrizione** |\n",
    "|------------------|----------------|\n",
    "| `chroma.sqlite3` | Database SQLite che contiene metadati e riferimenti agli embedding. |\n",
    "| `UUID/` | Cartella con ID univoco che contiene gli embedding veri e propri. |\n",
    "| `data_level0.bin` | File binario che contiene gli embedding memorizzati. |\n",
    "| `link_lists.bin` | Struttura di indicizzazione per la ricerca ANN. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Definizione della funzione di embedding personalizzata\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa classe?\n",
    "- Converte i documenti in **vettori numerici**.\n",
    "- ChromaDB **non genera embedding**, quindi ha bisogno di questa funzione per accettare testo e restituire vettori.\n",
    "\n",
    "### ðŸ“Œ Flusso dei dati\n",
    "1. Un documento (`input`) viene ricevuto.\n",
    "2. Viene convertito in embedding da `OllamaEmbeddings`.\n",
    "3. Lâ€™output Ã¨ una lista di vettori che possono essere memorizzati in ChromaDB.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Creazione della funzione di embedding\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- Inizializza **OllamaEmbeddings** per generare i vettori con `llama3.2`.\n",
    "- Usa `http://localhost:11434` per comunicare con il server Ollama.\n",
    "\n",
    "### ðŸ“Œ PerchÃ© Ã¨ importante?\n",
    "- **ChromaDB ha bisogno di un metodo esterno** per ottenere embedding.\n",
    "- Ollama genera i vettori che poi vengono salvati in `data_level0.bin`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Creazione della collezione in ChromaDB\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa riga?\n",
    "- Crea una **collezione** chiamata `\"rag_collection_demo_1\"`.\n",
    "- Se esiste giÃ , la recupera.\n",
    "- Associa la funzione di embedding (`embedding_function=embedding`).\n",
    "\n",
    "### ðŸ“Œ Struttura logica della collezione\n",
    "| **Campo** | **Contenuto** |\n",
    "|-----------|--------------|\n",
    "| `id` | ID univoco del documento. |\n",
    "| `embedding` | Vettore numerico associato al documento. |\n",
    "| `document` | Testo originale. |\n",
    "| `metadata` | Informazioni aggiuntive (es. autore, fonte). |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Funzione per aggiungere documenti\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa funzione?\n",
    "- Riceve **documenti testuali** e **ID univoci**.\n",
    "- Genera embedding e li memorizza nella collezione.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Aggiunta di documenti di esempio\n",
    "\n",
    "### ðŸ“Œ Cosa succede qui?\n",
    "1. I documenti vengono trasformati in embedding.\n",
    "2. Gli embedding vengono salvati in `data_level0.bin`.\n",
    "3. `chroma.sqlite3` registra i metadati.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Funzione per interrogare ChromaDB\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa funzione?\n",
    "- Converte la query in un embedding.\n",
    "- Cerca i documenti **piÃ¹ simili** con **ANN + Cosine Similarity**.\n",
    "- Restituisce i documenti corrispondenti.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Funzione per interrogare Ollama\n",
    "\n",
    "### ðŸ“Œ Cosa fa questa funzione?\n",
    "- Invia una richiesta a Ollama.\n",
    "- Restituisce una risposta generata dal modello.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Pipeline RAG\n",
    "\n",
    "### ðŸ“Œ Fasi della pipeline\n",
    "1. **Recupera documenti** da ChromaDB.\n",
    "2. **Costruisce un prompt migliorato**.\n",
    "3. **Interroga Ollama per generare la risposta**.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Test della pipeline\n",
    "\n",
    "### ðŸ“Œ Cosa fa?\n",
    "- Esegue la pipeline completa su una query di esempio.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Conclusione\n",
    "Questo notebook ha mostrato come implementare un sistema **RAG** utilizzando **ChromaDB per la memorizzazione e il retrieval di documenti vettorializzati** e **Ollama per la generazione di risposte basate su contesto estratto**.\n",
    "\n",
    "Se vuoi eseguire il codice, assicurati di avere un server **Ollama attivo su `http://localhost:11434`** e che **ChromaDB sia correttamente installato**. ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=os.path.join(os.getcwd(), \"chroma_db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBEmbeddingFunction:\n",
    "    \"\"\"\n",
    "    Funzione personalizzata per generare embedding con Ollama.\n",
    "    \"\"\"\n",
    "    def __init__(self, langchain_embeddings):\n",
    "        self.langchain_embeddings = langchain_embeddings\n",
    "\n",
    "    def __call__(self, input):\n",
    "        # Se il testo Ã¨ una stringa singola, lo converte in lista\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        return self.langchain_embeddings.embed_documents(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = ChromaDBEmbeddingFunction(\n",
    "    OllamaEmbeddings(\n",
    "        model=llm_model,\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"rag_collection_demo_1\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"A collection for RAG with Ollama - Demo1\"},\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_collection(documents, ids):\n",
    "    \"\"\"\n",
    "    Aggiunge documenti alla collezione ChromaDB.\n",
    "    \"\"\"\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: doc1\n",
      "Add of existing embedding ID: doc2\n",
      "Add of existing embedding ID: doc3\n",
      "Insert of existing embedding ID: doc1\n",
      "Insert of existing embedding ID: doc2\n",
      "Insert of existing embedding ID: doc3\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Artificial intelligence is the simulation of human intelligence processes by machines.\",\n",
    "    \"Python is a programming language that lets you work quickly and integrate systems more effectively.\",\n",
    "    \"ChromaDB is a vector database designed for AI applications.\"\n",
    "]\n",
    "doc_ids = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "\n",
    "add_documents_to_collection(documents, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query_text, n_results=1):\n",
    "    \"\"\"\n",
    "    Cerca documenti pertinenti nella collezione ChromaDB.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results[\"documents\"], results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"\n",
    "    Invia una query a Ollama e restituisce la risposta.\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=llm_model)\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query_text):\n",
    "    \"\"\"\n",
    "    Pipeline RAG: recupera documenti e genera una risposta.\n",
    "    \"\"\"\n",
    "    retrieved_docs, metadata = query_chromadb(query_text)\n",
    "    context = \" \".join(retrieved_docs[0]) if retrieved_docs else \"No relevant documents found.\"\n",
    "\n",
    "    augmented_prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "    print(\"######## Augmented Prompt ########\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    response = query_ollama(augmented_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Augmented Prompt ########\n",
      "Context: Artificial intelligence is the simulation of human intelligence processes by machines.\n",
      "\n",
      "Question: What is an LLM?\n",
      "Answer:\n",
      "######## Response from LLM ########\n",
      " A Large Language Model (LLM) is a type of artificial intelligence (AI) that uses machine learning techniques to process and understand natural language. It's trained on massive amounts of text data, enabling it to generate human-like responses to a wide range of questions and tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is an LLM?\"\n",
    "response = rag_pipeline(query)\n",
    "print(\"######## Response from LLM ########\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_prove)",
   "language": "python",
   "name": "rag_prove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
